



<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Bert个人总结 | 七翼式 | 不对称</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="theme-color" content="#3F51B5">
  
  
  <meta name="keywords" content="深度学习,NLP,bert">
  <meta name="description" content="#简介 bert大名不必多提。作为动态词嵌入最有名的模型，解析的博客数不胜数。什么transformer之类的本文一概不提。 本文简单叙述一下bert代码。 然后有精力的话，再用其他几篇文章总结一下bert出现之后的其他改进模型。 #代码总览 代码位于bert Mode                 LastWriteTime         Length Name ----">
<meta property="og:type" content="article">
<meta property="og:title" content="Bert个人总结">
<meta property="og:url" content="http://localhost:5000/2023/05/03/Bert%E4%B8%AA%E4%BA%BA%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="七翼式">
<meta property="og:description" content="#简介 bert大名不必多提。作为动态词嵌入最有名的模型，解析的博客数不胜数。什么transformer之类的本文一概不提。 本文简单叙述一下bert代码。 然后有精力的话，再用其他几篇文章总结一下bert出现之后的其他改进模型。 #代码总览 代码位于bert Mode                 LastWriteTime         Length Name ----">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-05-03T05:32:01.000Z">
<meta property="article:modified_time" content="2023-05-03T09:27:32.000Z">
<meta property="article:author" content="el_psy">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="bert">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="七翼式" type="application/atom+xml">
  
  <meta name="summary" content="">
  <link rel="shortcut icon" href="/favicon/favicon.ico">
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head>

<body>
  <div id="loading" class="active"></div>

  <nav id="menu" class="hide" >
   <div class="inner flex-row-vertical">
  <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
      <i class="icon icon-lg icon-close"></i>
  </a>
  <div class="brand-wrap">
    <div class="brand">
      <a href="/" class="avatar"><img src="/img/logo.jpg"></a>
      <hgroup class="introduce">
        <h5 class="nickname">el_psy</h5>
        <a href="mailto:gaojingjingjing@outlook.com" title="gaojingjingjing@outlook.com" class="mail">gaojingjingjing@outlook.com</a>
      </hgroup>
    </div>
  </div>
  <div class="scroll-wrap flex-col">
    <ul class="nav">
      
          <li class="waves-block waves-effect">
            <a href="/"  >
              <i class="icon icon-lg icon-home"></i>
              主页
            </a>
          </li>
      
          <li class="waves-block waves-effect">
            <a href="/archives"  >
              <i class="icon icon-lg icon-archives"></i>
              archives
            </a>
          </li>
      
          <li class="waves-block waves-effect">
            <a href="/categories"  >
              <i class="icon icon-lg icon-th-list"></i>
              categories
            </a>
          </li>
      
          <li class="waves-block waves-effect">
            <a href="/tags"  >
              <i class="icon icon-lg icon-tags"></i>
              Tags
            </a>
          </li>
      
          <li class="waves-block waves-effect">
            <a href="https://github.com/el-psy" target="_blank" >
              <i class="icon icon-lg icon-github"></i>
              Github
            </a>
          </li>
      
          <li class="waves-block waves-effect">
            <a href="/404"  >
              <i class="icon icon-lg icon-link"></i>
              测试
            </a>
          </li>
      
    </ul>

    <footer class="footer">
  <p><a rel="license" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0;vertical-align:middle;" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFAAAAAPCAMAAABEF7i9AAAAllBMVEUAAAD///+rsapERER3d3eIiIjMzMzu7u4iIiKUmZO6v7rKzsoODg4RERFVVVUNDQ0NDg0PEA8zMzNLTEtbXltmZmZydnF9gn2AgICPkI+ZmZmqqqq7u7vFxsXIzMgNDQwZGRkgICAhISEkJSMnKCcuMC4xMzE5Ozk7PTtBQkFCQkJDQ0Nna2eGhoaHh4ezuLLGysbd3d1wVGpAAAAA4UlEQVR42q2T1xqCMAyFk7QsBQeKA9x7j/d/OSm22CpX0nzcpA1/T05aAOuBVkMAScQFHLnEwoCo2f1TnQIGoVMewjZEjVFN4GH1Ue1Cn2jWqwfsOOj6wDwGvotsl/c8lv7KIq1eLOsT0HMFHMIE/RZyHnlphryT9zyV+8WH5e8yQw3wnQvgAFxPTKUVi555SHR/lOfLMgVTeDlSfN+TaoUsiTyeIm+bCkHvCA2FUKG48LDtYBZBknsYP/G8NTw0gaaHyuQf4H5pecrB/FYCT2sL9zAfy1Xyjou6L8X2W7YcLyBZCRtnq/zfAAAAAElFTkSuQmCC" /></a></p>
  <p>七翼式 &copy; 2023</p>
  <p>Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme
  <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a></p>
  <a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-2x icon-rss-square"></i></a>
</footer>

  </div>
</div>

  </nav>
  <main id="main">
    <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">Bert个人总结</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input " autocomplete="off" placeholder="输入感兴趣的关键字">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
    </div>
</header>
<header class="content-header">
  <div class="container">
    <h1 class="author">Bert个人总结</h1>
    <h5 class="subtitle">
        
            <time datetime="2023-05-03T05:32:01.000Z" itemprop="datePublished" class="page-time">
  2023-05-03
</time>


        
    </h5>
  </div>
</header>

    <div class="container body-wrap">
      <article id="post-Bert个人总结" 
  class="article article-type-post" itemprop="blogPost">
    <div class="post-meta flex-row">
        
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/bert/" rel="tag">bert</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </div>
    <div class="post-body">
        <aside class="post-widget" id="post-widget">

            
            <nav class="post-toc-wrap" id="post-toc">
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="post-toc-number">1.</span> <span class="post-toc-text">简介</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E4%BB%A3%E7%A0%81%E6%80%BB%E8%A7%88"><span class="post-toc-number">2.</span> <span class="post-toc-text">代码总览</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="post-toc-number">3.</span> <span class="post-toc-text">数据处理</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">启动</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#main%E5%87%BD%E6%95%B0"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">main函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#create-training-instances%E5%87%BD%E6%95%B0"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">create_training_instances函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#write-instance-to-example-files%E5%87%BD%E6%95%B0"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">write_instance_to_example_files函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="post-toc-number">4.</span> <span class="post-toc-text">预训练</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#%E6%80%BB%E8%A7%88"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">总览</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#main%E5%87%BD%E6%95%B0"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">main函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#model-fn-builder%E5%87%BD%E6%95%B0"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">model_fn_builder函数</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#%E6%9C%80%E5%90%8E"><span class="post-toc-number">5.</span> <span class="post-toc-text">最后</span></a></li></ol>
            </nav>
            
        </aside>

        <div class="post-main">

            <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="简介"><a class="header-anchor" href="#简介">#</a>简介</h1>
<p>bert大名不必多提。作为动态词嵌入最有名的模型，解析的博客数不胜数。什么transformer之类的本文一概不提。<br>
本文简单叙述一下bert代码。<br>
然后有精力的话，再用其他几篇文章总结一下bert出现之后的其他改进模型。</p>
<h1 id="代码总览"><a class="header-anchor" href="#代码总览">#</a>代码总览</h1>
<p>代码位于<a target="_blank" rel="noopener" href="https://github.com/google-research/bert">bert</a></p>
<pre class="line-numbers language-none"><code class="language-none">Mode                 LastWriteTime         Length Name
----                 -------------         ------ ----
-a----         2023&#x2F;4&#x2F;29     18:28           1477 .gitignore
-a----         2023&#x2F;4&#x2F;29     18:28           1354 CONTRIBUTING.md
-a----         2023&#x2F;4&#x2F;29     18:28          11560 LICENSE
-a----         2023&#x2F;4&#x2F;29     18:28          51636 README.md
-a----         2023&#x2F;4&#x2F;29     18:28            631 __init__.py
-a----         2023&#x2F;4&#x2F;29     18:28          16944 create_pretraining_data.py
-a----         2023&#x2F;4&#x2F;29     18:28          14317 extract_features.py
-a----         2023&#x2F;4&#x2F;29     18:28          38908 modeling.py
-a----         2023&#x2F;4&#x2F;29     18:28           9468 modeling_test.py
-a----         2023&#x2F;4&#x2F;29     18:28          11545 multilingual.md
-a----         2023&#x2F;4&#x2F;29     18:28           6432 optimization.py
-a----         2023&#x2F;4&#x2F;29     18:28           1769 optimization_test.py
-a----         2023&#x2F;4&#x2F;29     18:28          67718 predicting_movie_reviews_with_bert_on_tf_hub.ipynb
-a----         2023&#x2F;4&#x2F;29     18:28            112 requirements.txt
-a----         2023&#x2F;4&#x2F;29     18:28          35764 run_classifier.py
-a----         2023&#x2F;4&#x2F;29     18:28          11740 run_classifier_with_tfhub.py
-a----         2023&#x2F;4&#x2F;29     18:28          19160 run_pretraining.py
-a----         2023&#x2F;4&#x2F;29     18:28          47815 run_squad.py
-a----         2023&#x2F;4&#x2F;29     18:28           4427 sample_text.txt
-a----         2023&#x2F;4&#x2F;29     18:28          12656 tokenization.py
-a----         2023&#x2F;4&#x2F;29     18:28           4726 tokenization_test.py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>其中<code>create_pretraining_data.py</code>用于处理数据，<code>modeling.py</code>是模型及其配置对象所在，<code>run_pretraining.py</code>是bert模型训练所用。</p>
<h1 id="数据处理"><a class="header-anchor" href="#数据处理">#</a>数据处理</h1>
<h2 id="启动"><a class="header-anchor" href="#启动">#</a>启动</h2>
<p>代码的<code>README.md</code>叙述了代码处理命令。<br>
其中使用了<code>tensorflow</code>的flags处理了命令行指令的参数。<br>
使用<code>tf.app.run()</code>启动了<code>main</code>函数。</p>
<h2 id="main函数"><a class="header-anchor" href="#main函数">#</a>main函数</h2>
<ol>
<li>使用tf.logging设置了日志级别</li>
<li>创建tokenizer。熟悉huggingface的人应该不陌生。</li>
<li>读取命令行指令参数中的输入文件名，放到<code>input_files</code>数组里。并在日志输出</li>
<li>使用<code>create_training_instances</code>函数创建<code>instances</code>数组</li>
<li>使用<code>write_instance_to_example_files</code>函数将<code>instances</code>数组存放到指令中的输入文件参数里。</li>
</ol>
<h2 id="create-training-instances函数"><a class="header-anchor" href="#create-training-instances函数">#</a>create_training_instances函数</h2>
<ol>
<li>随机打乱document</li>
<li>控制每条数据长度。以0.5的概率随机插入，如果是随机<code>is_random_next</code>值为True，否则为False</li>
<li>token中使用<code>[CLS]</code>和<code>[SEP]</code>分隔符</li>
<li>使用<code>TrainingInstance</code>创建<code>instance</code>并保存在<code>instances</code>数组里。</li>
</ol>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">instance <span class="token operator">=</span> TrainingInstance<span class="token punctuation">(</span>
    tokens<span class="token operator">=</span>tokens<span class="token punctuation">,</span> <span class="token comment"># token分词结果列表，其中已经包含mask了</span>
    segment_ids<span class="token operator">=</span>segment_ids<span class="token punctuation">,</span> <span class="token comment"># 对应token的列表，由0， 1组成，代表前后两个分句</span>
    is_random_next<span class="token operator">=</span>is_random_next<span class="token punctuation">,</span> <span class="token comment"># 与segment_ids对应。如果两个分句在原文中是连续的为False；如果是随机链接（概率0.5）的话为True</span>
    masked_lm_positions<span class="token operator">=</span>masked_lm_positions<span class="token punctuation">,</span> <span class="token comment"># mask词对应的index</span>
    masked_lm_labels<span class="token operator">=</span>masked_lm_labels<span class="token punctuation">)</span> <span class="token comment"># 与之对应的mask词的token</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="write-instance-to-example-files函数"><a class="header-anchor" href="#write-instance-to-example-files函数">#</a>write_instance_to_example_files函数</h2>
<p>主要就是将下面这些写入文件中</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">features <span class="token operator">=</span> collections<span class="token punctuation">.</span>OrderedDict<span class="token punctuation">(</span><span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"input_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"input_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span>input_mask<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"segment_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span>segment_ids<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"masked_lm_positions"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span>masked_lm_positions<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"masked_lm_ids"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span>masked_lm_ids<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"masked_lm_weights"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_float_feature<span class="token punctuation">(</span>masked_lm_weights<span class="token punctuation">)</span>
features<span class="token punctuation">[</span><span class="token string">"next_sentence_labels"</span><span class="token punctuation">]</span> <span class="token operator">=</span> create_int_feature<span class="token punctuation">(</span><span class="token punctuation">[</span>next_sentence_label<span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 上面的is_random_next</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="预训练"><a class="header-anchor" href="#预训练">#</a>预训练</h1>
<h2 id="总览"><a class="header-anchor" href="#总览">#</a>总览</h2>
<p>代码的<code>README.md</code>叙述了代码处理命令。<br>
其中使用了<code>tensorflow</code>的flags处理了命令行指令的参数。<br>
使用<code>tf.app.run()</code>启动了<code>main</code>函数。</p>
<h2 id="main函数"><a class="header-anchor" href="#main函数">#</a>main函数</h2>
<ol>
<li>修改tf的logging等级</li>
<li>检验命令行参数中的train和eval，要么是评估模型，要么是训练模型</li>
<li>生成bert的config</li>
<li>读取输入文件名</li>
<li>tpu相关，不明</li>
<li>以<code>model_fn_builder</code>函数生成<code>model_fn</code></li>
<li>训练或者评估，然后日志</li>
</ol>
<h2 id="model-fn-builder函数"><a class="header-anchor" href="#model-fn-builder函数">#</a>model_fn_builder函数</h2>
<ol>
<li>读取输入数据</li>
<li>使用模型</li>
<li>计算loss</li>
</ol>
<p>从loss入手</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token punctuation">(</span>masked_lm_loss<span class="token punctuation">,</span>
 masked_lm_example_loss<span class="token punctuation">,</span> masked_lm_log_probs<span class="token punctuation">)</span> <span class="token operator">=</span> get_masked_lm_output<span class="token punctuation">(</span>
     bert_config<span class="token punctuation">,</span> model<span class="token punctuation">.</span>get_sequence_output<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> model<span class="token punctuation">.</span>get_embedding_table<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
     masked_lm_positions<span class="token punctuation">,</span> masked_lm_ids<span class="token punctuation">,</span> masked_lm_weights<span class="token punctuation">)</span>

<span class="token punctuation">(</span>next_sentence_loss<span class="token punctuation">,</span> next_sentence_example_loss<span class="token punctuation">,</span>
 next_sentence_log_probs<span class="token punctuation">)</span> <span class="token operator">=</span> get_next_sentence_output<span class="token punctuation">(</span>
     bert_config<span class="token punctuation">,</span> model<span class="token punctuation">.</span>get_pooled_output<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> next_sentence_labels<span class="token punctuation">)</span>

total_loss <span class="token operator">=</span> masked_lm_loss <span class="token operator">+</span> next_sentence_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>明显看出来bert预训练的两大任务，mask词预测和下一句预测。<br>
从<code>modeling.py</code>中分析可知，</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">model<span class="token punctuation">.</span>get_sequence_output<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 获取bert中的encoder输出，也就是transformer的输出</span>
model<span class="token punctuation">.</span>get_embedding_table<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 获取bert第一步的输出</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>这里简单提一下bert的模型。<br>
第一步是根据词的id（一个int）在embdding层找到对应的向量，多个词组成一句话形成一个矩阵。<br>
第二步是将这个矩阵输入到一组transformer模型中，得到bert输出的词向量<br>
第三步时其他层，稍后再讲。</p>
<p>这里mask词预测任务用到了bert输出的词向量，和embedding层输出，没记错两者长度都是<code>bert_config.hidden_size</code><br>
具体上，先将词向量进行线性变换，然后与embedding层输出相乘，然后进行<code>log_softmax</code>，与mask位置进行比较（mask的one_hot），得到loss。</p>
<p>前后句预测任务。<br>
输入时的<code>segment_id</code>（前句为0，后句为1）分割了前后句。<br>
首先<code>modeling.py</code>第三步有一个<code>pooler</code>，将词向量输出的第一个词的向量进行线性变换并输出，得到<code>pooled_output</code></p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"pooler"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># We "pool" the model by simply taking the hidden state corresponding</span>
    <span class="token comment"># to the first token. We assume that this has been pre-trained</span>
    first_token_tensor <span class="token operator">=</span> tf<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sequence_output<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>pooled_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
        first_token_tensor<span class="token punctuation">,</span>
        config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>
        activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>tanh<span class="token punctuation">,</span>
        kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>config<span class="token punctuation">.</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>计算loss时将其进行线性变换为长度为2的向量。<br>
需要预测的值是数据预处理是的<code>is_random_next</code>。<br>
交叉熵loss。</p>
<h1 id="最后"><a class="header-anchor" href="#最后">#</a>最后</h1>
<p>想必这篇文章与之前读过的bert博客不同。<br>
这也是我的初衷。<br>
要写点别人没有的。<br>
想必读完这篇之后会对bert源码有初步的了解。<br>
对于bert模型如何训练会有更深的了解。<br>
知道两大任务到底如何实现的。<br>
之后我还会随便总结一下其他bert改进模型，不过那就是没什么营养人云亦云了。<br>
反正我也不想在此领域深究。<br>
饶过我的2GB显存的小笔记本吧。</p>



            </div>
            
<nav class="post-nav">
  

  
    <div class="waves-block waves-effect next fr">
      <a href="/2023/01/11/graphql-introspection/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">graphql-introspection</h4>
      </a>
    </div>
  
</nav>


            
            
        </div>
    </div>
</article>

    </div>
  </main>
<div class="mask" id="mask"></div>
<a href="#top" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>

<script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>


<script src="/js/main.js"></script>




<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<script type="text/template" id="search-tpl">
<li class="item">
    <a href="/{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</script>


<script src="/js/search.js"></script>










</body>
</html>
